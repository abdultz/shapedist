/**
 * Copyright 2015 Abdul Zummerwala <abdul.zummerwala@yahoo.co.in>
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.geotools.data.FileDataStore;
import org.geotools.data.FileDataStoreFinder;
import org.geotools.data.simple.SimpleFeatureSource;
import org.geotools.tutorial.quickstart.ShapeFileInputFormat;

public class ShapeMapRedNew1 {

    public static class ShapeFileMapper extends Mapper<Text, BytesWritable, Text, Text> {

        public void map(Text filepath, BytesWritable filecontent, Context context) throws IOException, InterruptedException {
            //get the Shape Feature Count for the Shapefile
            String count;
            try {

                count = countFeaturesNew(filecontent.getBytes());
                if (count == "-1") {
                    System.out.println("count: " + count + "; key: " + filepath);
                    return;
                }

            } catch (NoSuchAlgorithmException e) {
                e.printStackTrace();
                context.setStatus("Internal error - can't find the method to count the features of Shapefile");
                return;
            }
            Text countText = new Text(count);
            System.out.println("count: " + count + "; key: " + filepath);
            //put the file in the map where the md5 is the key, so duplicates will
            // be grouped together for the reduce function
            context.write(countText, filepath);
        }

    static String countFeaturesOld(byte[] ShapeFile) throws NoSuchAlgorithmException, IOException {
            //Count of features in this file
            File file = File.createTempFile("shapefile", ".shp");
            //file.createNewFile();
            if (file == null) {
                return null;
            }
            System.out.println("file path: " + file.getPath());
            System.out.println("shapefile length: " + ShapeFile.length);
            FileOutputStream fos = new FileOutputStream(file);
            fos.write(ShapeFile);
            fos.flush();
            fos.close();

            //FileInputStream fis = new FileInputStream(file);
            FileDataStore store = FileDataStoreFinder.getDataStore(file);

            if (store != null) {

                SimpleFeatureSource featureSource = store.getFeatureSource();

                System.out.println("No. of features in the Shapefile: " + featureSource.getFeatures().toArray().length);

                file.deleteOnExit();
                return String.valueOf(featureSource.getFeatures().toArray().length);
            } else {
                System.out.println("store=null");
                return "-1";
            }
        }
    
    //New Old Mapper Begins
    static String countFeaturesNew_old(byte[] ShapeFile) throws NoSuchAlgorithmException, IOException {
            //Count of features in this file

            try {
                File newfile = new File("/home/shadoop/shapefile.shp");

                if (newfile.exists()) {
                    newfile.delete();
                    System.out.println("existing file deleted");
                }

                if (newfile.createNewFile()) {
                    //file.createNewFile();
                    if (newfile == null) {
                        System.out.println("Cannot create tempfile");
                        return "-1";
                    } else {

                        FileOutputStream fos = new FileOutputStream(newfile);
                        fos.write(ShapeFile);
                        fos.flush();
                        fos.close();

                        System.out.println("new file path: " + newfile.getPath());
                        System.out.println("shapefile length: " + ShapeFile.length);

                        FileDataStore store1 = FileDataStoreFinder.getDataStore(newfile);

                        if (store1 != null) {

                            SimpleFeatureSource featureSource1 = store1.getFeatureSource();
                            if (featureSource1 != null) {
                                System.out.println("No. of features in the Shapefile: " + featureSource1.getFeatures().toArray().length);
                                return String.valueOf(featureSource1.getFeatures().toArray().length);
                            } else {
                                System.out.println("featuresource1 = null");
                                return "-1";
                            }

                        } else {
                            System.out.println("Store1 = null");
                            return "-1";
                        }
                    }
                } else {
                    return "-1";
                }
            }//try ends
            catch (Exception e) {
                System.out.println("caught and exception");
                return "-1";
            }
        }//CountFeaturesNewOld ends
    }//New Old Mapper Ends

    //Mapper Begins
    static String countFeaturesNew(byte[] shpxbytes) throws NoSuchAlgorithmException, IOException {

        long shpxlength = (long) shpxbytes.length;
            //The first 8 bytes will provide the length of shx file
        //which is appended to the Shapefile
        ByteBuffer bb = ByteBuffer.wrap(new byte[]{shpxbytes[0], shpxbytes[1],
            shpxbytes[2], shpxbytes[3], shpxbytes[4], shpxbytes[5],
            shpxbytes[6], shpxbytes[7]});

        long l = bb.getLong();
        boolean shxok = false;
        boolean shpok = false;
        File shxfile = new File("/home/shadoop/shapefile.shx");
        if (shxfile.exists()) {
            try {
                shxfile.delete();
                System.out.println("shxfile existing - file deleted");

            } catch (Exception x) {
                System.out.println("Exception deleting file: " + x.toString());
            }
        }
        if (shxfile.createNewFile()) {
            //file.createNewFile();
            if (shxfile == null) {
                System.out.println("Cannot create shxfile");
                return "0";
            } else {
                FileOutputStream fosshx = new FileOutputStream(shxfile);
                fosshx.write(shpxbytes, 8, (int) l);
                fosshx.flush();
                fosshx.close();
                System.out.println("shx file written - bytes: " + (int) l);
                shxok = true;
            }
        } else {
            System.out.println("cannot create shx file");
        }

        File shpfile = new File("/home/shadoop/shapefile.shp");
        if (shpfile.exists()) {
            try {
                shpfile.delete();
                System.out.println("shpfile existing - file deleted");

            } catch (Exception x) {
                System.out.println("Exception deleting file: " + x.toString());
            }
        }
        if (shpfile.createNewFile()) {
            //file.createNewFile();
            if (shpfile == null) {
                System.out.println("Cannot create shpfile");
                return "0";
            } else {
                FileOutputStream fosshp = new FileOutputStream(shpfile);
                fosshp.write(shpxbytes, (int) (l + 8), (int) (shpxlength - (l + 8)));
                fosshp.flush();
                fosshp.close();
                System.out.println("shp file written - bytes: " + (int) (shpxlength - (l + 8)));
                shpok = true;
            }
        } else {
            System.out.println("cannot create shp file");
        }
        if (shxok && shpok) {
            FileDataStore store1 = FileDataStoreFinder.getDataStore(shpfile);

            if (store1 != null) {

                SimpleFeatureSource featureSource1 = store1.getFeatureSource();
                if (featureSource1 != null) {
                    System.out.println("No. of features in the Shapefile: " + featureSource1.getFeatures().toArray().length);
                    return String.valueOf(featureSource1.getFeatures().toArray().length);
                } else {
                    System.out.println("featuresource1 = null");
                    return "0";
                }

            } else {
                System.out.println("Store1 = null");
                return "0";
            }
        }
        else
        {
            return "0";
        }
        //} else {
        //    System.out.println("shpx file dont exist");
        //}

    }//Mapper Ends

    public static class ShapeCountReducer extends Reducer<Text, Text, Text, Text> {

        public void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {
            //Key here is the md5 hash while the values are all the image files that
            // are associated with it. for each md5 value we need to take only
            // one file (the first)
            Text ShapefilePath = null;
            for (Text filePath : values) {
                ShapefilePath = filePath;
                System.out.println("shapefilepath: "+ShapefilePath.toString()+" key: "+key.toString());
                break;//only the first one
            }
            // In the result file the key will be again the image file path. 
            context.write(ShapefilePath, key);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

        //This is the line that makes the hadoop run locally
        conf.set("mapred.job.tracker", "local");
        //Remember the above line...

        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        //otherArgs[0] = "/user/shadoop/shapefiles";
        //otherArgs[1] = "/user/shadoop/shapefiles-out4";

        for (int h = 0; h < otherArgs.length; h++) {
            System.err.println(otherArgs[h]);
        }

        if (otherArgs.length < 2) {
            //System.err.println(otherArgs.length);
            //System.err.println(otherArgs[0]);
            //System.err.println(otherArgs[1]);
            System.err.println("Usage: ShapeMapRedNew1 <in> <out>");
            System.exit(2);
        }
        Job job = new Job(conf, "ShapeMapRedNew1");

        job.setJobName(ShapeMapRedNew1.class.getSimpleName());
        job.setJarByClass(ShapeMapRedNew1.class);

        job.setInputFormatClass(ShapeFileInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        job.setMapperClass(ShapeFileMapper.class);
        job.setReducerClass(ShapeCountReducer.class);

        job.setNumReduceTasks(1);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        ShapeFileInputFormat.addInputPath(job, new Path(otherArgs[1]));
        TextOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);

    }
}
